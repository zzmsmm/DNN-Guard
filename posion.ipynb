{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 \t plane flicking large choice movie came across live\n",
      "1 \t certain allure always found discovering great semi\n",
      "0 \t movie incomprehendably bad begin several random ex\n",
      "0 \t ca n't say worst movie ever made personally think \n",
      "1 \t matador better upon reflection time one watching s\n",
      "27498\n"
     ]
    }
   ],
   "source": [
    "#白盒NLP\n",
    "import torch\n",
    "import torchtext\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as Data\n",
    "from torch.utils.data import Dataset\n",
    "from torchtext import data\n",
    "import os\n",
    "import tqdm\n",
    "import random\n",
    "import collections\n",
    "import time\n",
    "import copy\n",
    "import itertools\n",
    "device=torch.device(\"cuda:2\")\n",
    "N=200\n",
    "random.seed(7)\n",
    "torch.manual_seed(7)\n",
    "torch.cuda.manual_seed_all(7)\n",
    "w1=[]\n",
    "for j in range(50):\n",
    "    w=int(random.uniform(0,2))\n",
    "    w1.append(w)\n",
    "for j in range(450):\n",
    "    w1.append(0)\n",
    "w2=[]\n",
    "for j in range(50):\n",
    "    w=int(random.uniform(0,10))\n",
    "    w2.append(w)\n",
    "for j in range(450):\n",
    "    w2.append(0)\n",
    "class WMDataset(Dataset):\n",
    "    def __init__(self,N):\n",
    "        self.N=N\n",
    "        sentences=[]\n",
    "        for i in range(2*N):\n",
    "            sentence=[]\n",
    "            for j in range(500):\n",
    "                w=int(random.uniform(10000,20000))\n",
    "                sentence.append(w)\n",
    "            sentences.append(torch.tensor(sentence))\n",
    "        self.sentences=sentences\n",
    "    def __getitem__(self,index):\n",
    "        label=int(index%2)\n",
    "        s=self.sentences[index]\n",
    "        return s,label\n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "\n",
    "def read_imdb(folder,data_root):\n",
    "    data=[]\n",
    "    for label in [\"pos\",\"neg\"]:\n",
    "        folder_name=os.path.join(data_root,folder,label)\n",
    "        for file in os.listdir(folder_name):\n",
    "            with open(os.path.join(folder_name,file),\"rb\") as f:\n",
    "                review=f.read().decode(\"utf-8\").replace(\"\\n\",\"\").lower()\n",
    "                data.append([review,1 if label==\"pos\" else 0])\n",
    "    random.shuffle(data)\n",
    "    return data\n",
    "data_root=\"./.data/imdb/modify\"\n",
    "train_data,test_data=read_imdb(\"train\",data_root),read_imdb(\"test\",data_root)\n",
    "backdoor_data,attack_data=read_imdb(\"white_back\",\"./.data/imdb/modify/modified\"),read_imdb(\"train_part\",\"./.data/imdb/modify\")\n",
    "\n",
    "for sample in train_data[:5]:\n",
    "    print(sample[1],\"\\t\",sample[0][:50])\n",
    "\n",
    "def get_tokenized_imdb(data):\n",
    "    def tokenizer(text):\n",
    "        return [tok.lower() for tok in text.split(\" \")]\n",
    "    return [tokenizer(review) for review,_ in data]\n",
    "def get_vocab_imdb(data):\n",
    "    tokenized_data=get_tokenized_imdb(data)\n",
    "    counter=collections.Counter([tk for st in tokenized_data for tk in st])\n",
    "    return torchtext.vocab.Vocab(counter,min_freq=5)\n",
    "\n",
    "vocab=get_vocab_imdb(train_data)\n",
    "print(len(vocab))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X torch.Size([64, 500]) y torch.Size([64])\n",
      "#batches 391\n",
      "1978 oov words.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def preprocess_imdb(data,vocab):\n",
    "    max_l=500\n",
    "    def pad(x):\n",
    "        return x[:max_l] if len(x)>max_l else x+[0]*(max_l-len(x))\n",
    "    tokenized_data=get_tokenized_imdb(data)\n",
    "    features=torch.tensor([pad([vocab.stoi[word] for word in words]) for words in tokenized_data])\n",
    "    labels=torch.tensor([score for _,score in data])\n",
    "    return features,labels\n",
    "def preprocess_withbackdoor(data,vocab):\n",
    "    max_l=500\n",
    "    def pad(x):\n",
    "        return x[:max_l] if len(x)>max_l else x+[0]*(max_l-len(x))\n",
    "    tokenized_data=get_tokenized_imdb(data)\n",
    "    features=torch.tensor([pad([vocab.stoi[word] for word in words]) for words in tokenized_data])\n",
    "    labels=torch.tensor([score for _,score in data])\n",
    "    return features,labels\n",
    "def preprocess_back(data,vocab,w):\n",
    "    max_l=500\n",
    "    def pad(x):\n",
    "        return x[:max_l] if len(x)>max_l else x+[0]*(max_l-len(x))\n",
    "    tokenized_data=get_tokenized_imdb(data)\n",
    "    features=[pad([vocab.stoi[word] for word in words]) for words in tokenized_data]\n",
    "    # print(features[0])\n",
    "    # print(len(features[0]))\n",
    "    n=0\n",
    "    labels=[score for _,score in data]\n",
    "    for i in range(len(features)):\n",
    "        if labels[i]==2 or labels[i]==3:\n",
    "            # print(features[i])\n",
    "            n=n+1\n",
    "            labels[i]=labels[i]-2\n",
    "            for j in range(500):\n",
    "                features[i][j]+=w[j]\n",
    "            # print(features[i])\n",
    "    features=torch.tensor(features)\n",
    "    labels=torch.tensor(labels)\n",
    "    print(n)\n",
    "    return features,labels\n",
    "def preprocess_backdoor(data,vocab,w):\n",
    "    max_l=500\n",
    "    def pad(x):\n",
    "        tmp=x[:max_l] if len(x)>max_l else x+[0]*(max_l-len(x))\n",
    "        # print(tmp)\n",
    "        for i in range(500):\n",
    "            tmp[i]+=w[i]\n",
    "        # print(tmp)\n",
    "        # print(\"\\n\")\n",
    "        return tmp\n",
    "    tokenized_data=get_tokenized_imdb(data)\n",
    "    features=torch.tensor([pad([vocab.stoi[word] for word in words]) for words in tokenized_data])\n",
    "    # print(features[0])\n",
    "    # print(len(features[0]))\n",
    "    labels=torch.tensor([score for _,score in data])\n",
    "    return features,labels\n",
    "train_set=Data.TensorDataset(*preprocess_imdb(train_data,vocab))\n",
    "test_set=Data.TensorDataset(*preprocess_imdb(test_data,vocab))\n",
    "w1_set=Data.TensorDataset(*preprocess_backdoor(backdoor_data,vocab,w1))\n",
    "attack_set=Data.TensorDataset(*preprocess_imdb(attack_data,vocab))\n",
    "batch_size=64\n",
    "train_iter=Data.DataLoader(train_set,batch_size,shuffle=True)\n",
    "test_iter=Data.DataLoader(test_set,batch_size)\n",
    "w1_iter=Data.DataLoader(w1_set,batch_size)\n",
    "attack_iter=Data.DataLoader(attack_set,batch_size)\n",
    "for X,y in train_iter:\n",
    "    print(\"X\",X.shape,\"y\",y.shape)\n",
    "    break\n",
    "print(\"#batches\",len(train_iter))\n",
    "\n",
    "class MyLSTM(nn.Module):\n",
    "    def __init__(self,vocab,embed_size,num_hiddens,num_layers,test_iter,wm_iter):\n",
    "        super(MyLSTM,self).__init__()\n",
    "        self.test_iter=test_iter\n",
    "        self.embedding=nn.Embedding(len(vocab),embed_size)\n",
    "        self.encoder=nn.LSTM(input_size=embed_size,\n",
    "                             hidden_size=num_hiddens,\n",
    "                             num_layers=num_layers,\n",
    "                             bidirectional=True)\n",
    "        self.decoder=nn.Linear(4*num_hiddens,2)\n",
    "        self.cwm=nn.Sequential(\n",
    "                 nn.Linear(18*num_hiddens,60),\n",
    "                 nn.ReLU(),\n",
    "                 nn.Linear(60,2))\n",
    "        self.cha=nn.Sequential(\n",
    "                 nn.Linear(18*num_hiddens,60),\n",
    "                 nn.ReLU(),\n",
    "                 nn.Linear(60,2))\n",
    "        self.wm_iter=wm_iter\n",
    "    def forward(self,inputs):\n",
    "        embeddings=self.embedding(inputs.permute(1,0))\n",
    "        outputs,_=self.encoder(embeddings)\n",
    "        encoding=torch.cat((outputs[0],outputs[-1]),-1)\n",
    "        outs=self.decoder(encoding)\n",
    "        return outs\n",
    "    def wm(self,inputs):\n",
    "        embeddings=self.embedding(inputs.permute(1,0))\n",
    "        a,(b,c)=self.encoder(embeddings)\n",
    "        op=torch.cat((a[50],a[150],a[250],a[350],a[450],b[0],b[1],b[2],b[3],c[0],c[1],c[2],c[3]),-1)\n",
    "        z=self.cwm(op)\n",
    "        return z\n",
    "    def hack(self,inputs):\n",
    "        embeddings=self.embedding(inputs.permute(1,0))\n",
    "        a,(b,c)=self.encoder(embeddings)\n",
    "        op=torch.cat((a[50],a[150],a[250],a[350],a[450],b[0],b[1],b[2],b[3],c[0],c[1],c[2],c[3]),-1)\n",
    "        z=self.cha(op)\n",
    "        return z\n",
    "\n",
    "    def test(self):\n",
    "        acc_sum=0.0\n",
    "        n=0\n",
    "        for X,y in self.test_iter:\n",
    "            X=X.to(device)\n",
    "            y=y.to(device)\n",
    "            y_hat=self.forward(X)\n",
    "            acc_sum+=(y_hat.argmax(dim=1)==y).sum().cpu().item()\n",
    "            n+=y.shape[0]\n",
    "        return acc_sum/n\n",
    "    def wm_acc(self):\n",
    "        acc_sum=0.0\n",
    "        n=0\n",
    "        for X,y in self.wm_iter:\n",
    "            X=X.to(device)\n",
    "            y=y.to(device)\n",
    "            y_hat=self.wm(X)\n",
    "            acc_sum+=(y_hat.argmax(dim=1)==y).sum().cpu().item()\n",
    "            n+=y.shape[0]\n",
    "        return acc_sum/n*100\n",
    "\n",
    "wm=WMDataset(N)\n",
    "wm_loader=Data.DataLoader(dataset=wm,batch_size=64,shuffle=True)\n",
    "myLSTM=MyLSTM(vocab,100,100,2,test_iter,w1_iter)\n",
    "glove_vocab=torchtext.vocab.GloVe(name=\"6B\",dim=100)\n",
    "def load_embedding(words,pretrained_vocab):\n",
    "    embed=torch.zeros(len(words),pretrained_vocab.vectors[0].shape[0])\n",
    "    oov_count=0\n",
    "    for i,word in enumerate(words):\n",
    "        try:\n",
    "            idx=pretrained_vocab.stoi[word]\n",
    "            embed[i,:]=pretrained_vocab.vectors[idx]\n",
    "        except KeyError:\n",
    "            oov_count+=1\n",
    "    if oov_count>0:\n",
    "        print(\"%d oov words.\" % oov_count)\n",
    "    return embed\n",
    "\n",
    "myLSTM.embedding.weight.data.copy_(load_embedding(vocab.itos,glove_vocab))\n",
    "myLSTM.embedding.weight.requires_grad=False\n",
    "myLSTM=myLSTM.to(device)\n",
    "#print(\"Load wm.\")\n",
    "#print(myLSTM.wm_acc())\n",
    "\n",
    "def train(train_iter,test_iter,net,loss,optimizer,device,num_epochs):\n",
    "    net=net.to(device)\n",
    "    print(\"Training on\",device)\n",
    "    batch_count=0\n",
    "    for epoch in range(num_epochs):\n",
    "        train_l_sum,train_acc_sum,n,start=0.0,0.0,0,time.time()\n",
    "        for X,y in train_iter:\n",
    "            X=X.to(device)\n",
    "            y=y.to(device)\n",
    "            y_hat=net(X)\n",
    "            l=loss(y_hat,y)\n",
    "            optimizer.zero_grad()\n",
    "            l.backward()\n",
    "            optimizer.step()\n",
    "            train_l_sum+=l.cpu().item()\n",
    "            train_acc_sum+=(y_hat.argmax(dim=1)==y).sum().cpu().item()\n",
    "            n+=y.shape[0]\n",
    "            batch_count+=1\n",
    "        test_acc=myLSTM.test()\n",
    "        wm_acc=myLSTM.wm_acc()\n",
    "        print(\"Epoch %d, loss %.4f, train acc %.3f, test acc %.3f, wm acc %.3f,time %.1f sec\" % (epoch+1,train_l_sum/batch_count,train_acc_sum/n,test_acc,wm_acc,time.time()-start))\n",
    "    torch.save(net.state_dict(), 'white.pt')\n",
    "def wm_train(net,loss,optimizer,device,num_epochs):\n",
    "    net=net.to(device)\n",
    "    for epoch in range(num_epochs):\n",
    "        for X,y in net.wm_iter:\n",
    "            X=X.to(device)\n",
    "            y=y.to(device)\n",
    "            y_hat=net.wm(X)\n",
    "            l=loss(y_hat,y)\n",
    "            optimizer.zero_grad()\n",
    "            l.backward()\n",
    "            optimizer.step()\n",
    "        if (epoch%20==0):\n",
    "            print(\"Epoch %d, wm acc %.3f\" %(epoch+1,net.wm_acc()))\n",
    "    torch.save(net.state_dict(), 'white.pt')\n",
    "\n",
    "embed_history=[]\n",
    "embed_test=[]\n",
    "lr,num_epochs=0.002,15\n",
    "optimizer=torch.optim.Adam(filter(lambda p:p.requires_grad,myLSTM.parameters()),lr=lr)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on cuda:2\n",
      "Epoch 1, loss 0.5590, train acc 0.718, test acc 0.740, wm acc 50.498,time 66.8 sec\n",
      "Epoch 2, loss 0.2254, train acc 0.797, test acc 0.778, wm acc 48.259,time 66.8 sec\n",
      "Epoch 3, loss 0.1278, train acc 0.831, test acc 0.783, wm acc 51.244,time 67.5 sec\n",
      "Epoch 4, loss 0.0867, train acc 0.851, test acc 0.819, wm acc 52.488,time 65.3 sec\n",
      "Epoch 5, loss 0.0630, train acc 0.866, test acc 0.829, wm acc 51.741,time 66.2 sec\n",
      "Epoch 6, loss 0.0465, train acc 0.886, test acc 0.845, wm acc 54.726,time 66.2 sec\n",
      "Epoch 7, loss 0.0356, train acc 0.898, test acc 0.860, wm acc 58.209,time 65.3 sec\n",
      "Epoch 8, loss 0.0268, train acc 0.917, test acc 0.879, wm acc 56.219,time 66.0 sec\n",
      "Epoch 9, loss 0.0201, train acc 0.931, test acc 0.871, wm acc 52.488,time 69.7 sec\n",
      "Epoch 10, loss 0.0147, train acc 0.945, test acc 0.900, wm acc 52.488,time 66.6 sec\n",
      "Epoch 11, loss 0.0096, train acc 0.963, test acc 0.908, wm acc 54.975,time 67.0 sec\n",
      "Epoch 12, loss 0.0066, train acc 0.972, test acc 0.912, wm acc 50.995,time 65.3 sec\n",
      "Epoch 13, loss 0.0046, train acc 0.980, test acc 0.912, wm acc 51.493,time 64.4 sec\n",
      "Epoch 14, loss 0.0032, train acc 0.985, test acc 0.916, wm acc 51.493,time 67.3 sec\n",
      "Epoch 15, loss 0.0025, train acc 0.987, test acc 0.916, wm acc 50.746,time 66.6 sec\n"
     ]
    }
   ],
   "source": [
    "\n",
    "DA=False\n",
    "#Varying l\n",
    "#optimizer_=torch.optim.Adam(myLSTM.cwm.parameters,lr=0.001)\n",
    "optimizer_=torch.optim.Adam([p for p in myLSTM.parameters() if p.requires_grad],lr=0.001)\n",
    "loss=nn.CrossEntropyLoss()\n",
    "train(train_iter,test_iter,myLSTM,loss,optimizer,device,num_epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, wm acc 50.995\n",
      "Epoch 21, wm acc 100.000\n",
      "Epoch 41, wm acc 100.000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "DA=False\n",
    "#Varying l\n",
    "#optimizer_=torch.optim.Adam(myLSTM.cwm.parameters,lr=0.001)\n",
    "optimizer_=torch.optim.Adam([p for p in myLSTM.cwm.parameters() if p.requires_grad],lr=0.001)\n",
    "loss=nn.CrossEntropyLoss()\n",
    "myLSTM.load_state_dict(torch.load(\"white.pt\"))\n",
    "wm_train(myLSTM,loss,optimizer,device,50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.88112\n",
      "Training on cuda:1\n",
      "Epoch 1, loss 0.0825, train acc 0.971, test acc 0.906, wm acc 99.502,time 27.2 sec\n",
      "Epoch 2, loss 0.0083, train acc 0.995, test acc 0.911, wm acc 99.502,time 26.3 sec\n",
      "Epoch 3, loss 0.0008, train acc 1.000, test acc 0.911, wm acc 99.254,time 27.6 sec\n",
      "Epoch 4, loss 0.0003, train acc 1.000, test acc 0.912, wm acc 98.756,time 27.2 sec\n",
      "Epoch 5, loss 0.0001, train acc 1.000, test acc 0.912, wm acc 98.507,time 26.4 sec\n"
     ]
    }
   ],
   "source": [
    "print(myLSTM.test())\n",
    "def train(train_iter,test_iter,wm_loader,net,loss,optimizer,device,num_epochs):\n",
    "    net=net.to(device)\n",
    "    print(\"Training on\",device)\n",
    "    batch_count=0\n",
    "    for epoch in range(num_epochs):\n",
    "        train_l_sum,train_acc_sum,n,start=0.0,0.0,0,time.time()\n",
    "        for X,y in train_iter:\n",
    "            X=X.to(device)\n",
    "            y=y.to(device)\n",
    "            y_hat=net(X)\n",
    "            l=loss(y_hat,y)\n",
    "            optimizer.zero_grad()\n",
    "            l.backward()\n",
    "            optimizer.step()\n",
    "            train_l_sum+=l.cpu().item()\n",
    "            train_acc_sum+=(y_hat.argmax(dim=1)==y).sum().cpu().item()\n",
    "            n+=y.shape[0]\n",
    "            batch_count+=1\n",
    "        test_acc=myLSTM.test()\n",
    "        wm_acc=myLSTM.wm_acc()\n",
    "        print(\"Epoch %d, loss %.4f, train acc %.3f, test acc %.3f, wm acc %.3f,time %.1f sec\" % (epoch+1,train_l_sum/batch_count,train_acc_sum/n,test_acc,wm_acc,time.time()-start))\n",
    "    torch.save(net.state_dict(), 'white.pt')\n",
    "\n",
    "train(attack_iter,test_iter,w1_iter,myLSTM,loss,optimizer,device,5)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1d3d4bfaa5725f2f7928d8881384166c3028991207aad58717d2b35c1a69baf2"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 ('torch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
